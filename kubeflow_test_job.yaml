apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-dist-test
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          hostNetwork: true
          # We need to tolerate the node taints if any (K3s usually doesn't taint unless requested)
          containers:
            - name: pytorch
              image: ray_nccl_node:latest
              imagePullPolicy: Never
              # Override Ray entrypoint to run our custom shell command
              command: ["/bin/bash", "-c"]
              args:
                - |
                  echo "Master started on $(hostname)"
                  echo "IP: $(hostname -I)"
                  
                  # Attempt to auto-detect the interface used for the default route
                  export NCCL_SOCKET_IFNAME=$(ip route get 1 | awk '{print $5;exit}')
                  echo "Detected Interface: $NCCL_SOCKET_IFNAME"
                  
                  # Run a simple Distributed PyTorch test inline
                  python3 -c "
                  import os
                  import torch
                  import torch.distributed as dist
                  
                  rank = int(os.environ['RANK'])
                  world_size = int(os.environ['WORLD_SIZE'])
                  
                  print(f'Rank {rank}/{world_size} initializing...')
                  if torch.cuda.is_available():
                      torch.cuda.set_device(0)
                      device = torch.device('cuda:0')
                      print(f'Rank {rank} using CUDA: {torch.cuda.get_device_name(0)}')
                  else:
                      device = torch.device('cpu')
                      print(f'Rank {rank} using CPU')

                  dist.init_process_group(backend='nccl' if torch.cuda.is_available() else 'gloo')
                  
                  tensor = torch.ones(1).to(device) * (rank + 1)
                  dist.all_reduce(tensor)
                  print(f'Rank {rank} result: {tensor.item()} (Expected: {world_size * (world_size + 1) / 2})')
                  
                  dist.destroy_process_group()
                  print(f'Rank {rank} Finished.')
                  "
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          hostNetwork: true
          containers:
            - name: pytorch
              image: ray_nccl_node:latest
              imagePullPolicy: Never
              command: ["/bin/bash", "-c"]
              args:
                - |
                  echo "Worker started on $(hostname)"
                  echo "IP: $(hostname -I)"
                  export NCCL_SOCKET_IFNAME=$(ip route get 1 | awk '{print $5;exit}')
                  echo "Detected Interface: $NCCL_SOCKET_IFNAME"
                  
                  python3 -c "
                  import os
                  import torch
                  import torch.distributed as dist
                  
                  rank = int(os.environ['RANK'])
                  world_size = int(os.environ['WORLD_SIZE'])
                  
                  print(f'Rank {rank}/{world_size} initializing...')
                  if torch.cuda.is_available():
                      torch.cuda.set_device(0)
                      device = torch.device('cuda:0')
                  else:
                      device = torch.device('cpu')

                  dist.init_process_group(backend='nccl' if torch.cuda.is_available() else 'gloo')
                  
                  tensor = torch.ones(1).to(device) * (rank + 1)
                  dist.all_reduce(tensor)
                  
                  dist.destroy_process_group()
                  print(f'Rank {rank} Finished.')
                  "
