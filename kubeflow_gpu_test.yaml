apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: pytorch-gpu-test
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          hostNetwork: true
          containers:
            - name: pytorch
              image: ray_nccl_node:latest
              imagePullPolicy: Never
              resources:
                limits:
                  nvidia.com/gpu: 1
              command: ["/bin/bash", "-c"]
              args:
                - |
                  set -e
                  set -x
                  
                  # Create the python script
                  cat << 'EOF' > /tmp/test.py
                  import torch
                  import torch.distributed as dist
                  import os
                  import socket
                  import sys

                  rank = int(os.environ.get('RANK', 0))
                  ws = int(os.environ.get('WORLD_SIZE', 1))
                  host = socket.gethostname()

                  print(f'[{host}] Rank {rank}/{ws} Initializing...')

                  if not torch.cuda.is_available():
                      print(f'[{host}] ERROR: CUDA not available')
                      sys.exit(1)
                      
                  device = torch.device('cuda:0')
                  print(f'[{host}] Using GPU: {torch.cuda.get_device_name(device)}')

                  # Initialize Process Group
                  # We use nccl backend as requested
                  dist.init_process_group(backend='nccl')

                  val = 1.0
                  tensor = torch.tensor([val]).to(device)
                  print(f'[{host}] Local Tensor: {tensor.item()}')

                  # All Reduce (Sum)
                  dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
                  result = tensor.item()

                  expected = val * ws
                  print(f'[{host}] Result: {result}')

                  dist.destroy_process_group()

                  if abs(result - expected) < 1e-6:
                      print(f'[{host}] VALIDATION SUCCESS: {result} == {expected}')
                  else:
                      print(f'[{host}] VALIDATION FAILURE: {result} != {expected}')
                      sys.exit(1)
                  EOF

                  # Auto-detect interface for NCCL
                  # We use python to get the IP to avoid dependency on 'ip' command
                  export NCCL_SOCKET_IFNAME=$(python3 -c "import socket; print(socket.gethostbyname(socket.gethostname()))" 2>/dev/null)
                  # If that fails (e.g. returns 127.0.0.1), try a route to internet
                  if [[ "$NCCL_SOCKET_IFNAME" == "127.0.0.1" || -z "$NCCL_SOCKET_IFNAME" ]]; then
                      export NCCL_SOCKET_IFNAME=$(python3 -c "import socket; s=socket.socket(socket.AF_INET, socket.SOCK_DGRAM); s.connect(('8.8.8.8', 80)); print(s.getsockname()[0]); s.close()" 2>/dev/null)
                  fi
                  echo "Detected NCCL Interface: $NCCL_SOCKET_IFNAME"

                  # Debug: Check GPU visibility
                  echo ">>> Checking GPU visibility:"
                  nvidia-smi
                  
                  # Run with UV, forcing CUDA torch
                  echo "Running test with uv (CUDA)..."
                  uv run --with "torch" --extra-index-url https://download.pytorch.org/whl/cu124 /tmp/test.py
    Worker:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          hostNetwork: true
          containers:
            - name: pytorch
              image: ray_nccl_node:latest
              imagePullPolicy: Never
              resources:
                limits:
                  nvidia.com/gpu: 1
              command: ["/bin/bash", "-c"]
              args:
                - |
                  set -e
                  set -x
                  
                  cat << 'EOF' > /tmp/test.py
                  import torch
                  import torch.distributed as dist
                  import os
                  import socket
                  import sys

                  rank = int(os.environ.get('RANK', 0))
                  ws = int(os.environ.get('WORLD_SIZE', 1))
                  host = socket.gethostname()

                  print(f'[{host}] Rank {rank}/{ws} Initializing...')

                  if not torch.cuda.is_available():
                      print(f'[{host}] ERROR: CUDA not available')
                      sys.exit(1)
                      
                  device = torch.device('cuda:0')
                  print(f'[{host}] Using GPU: {torch.cuda.get_device_name(device)}')

                  dist.init_process_group(backend='nccl')

                  val = 1.0
                  tensor = torch.tensor([val]).to(device)
                  print(f'[{host}] Local Tensor: {tensor.item()}')

                  dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
                  result = tensor.item()

                  expected = val * ws
                  print(f'[{host}] Result: {result}')

                  dist.destroy_process_group()

                  if abs(result - expected) < 1e-6:
                      print(f'[{host}] VALIDATION SUCCESS: {result} == {expected}')
                  else:
                      print(f'[{host}] VALIDATION FAILURE: {result} != {expected}')
                      sys.exit(1)
                                    EOF
                  
                                    export NCCL_SOCKET_IFNAME=$(python3 -c "import socket; s=socket.socket(socket.AF_INET, socket.SOCK_DGRAM); s.connect(('8.8.8.8', 80)); print(s.getsockname()[0]); s.close()" 2>/dev/null)
                                                                        echo "Detected NCCL Interface: $NCCL_SOCKET_IFNAME"
                                                                        
                                                                        echo ">>> Checking GPU visibility:"
                                                                        nvidia-smi
                                                      
                                                                        echo "Running test with uv (CUDA)..."
                                                                        uv run --with "torch" --extra-index-url https://download.pytorch.org/whl/cu124 /tmp/test.py
                                                                        